{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, node_id, features=None):\n",
    "        self.node_id = node_id\n",
    "        self.adj_list = []\n",
    "        self.features = features if features else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "\n",
    "    def add_node(self, node_id, features=None):\n",
    "        if node_id not in self.nodes:\n",
    "            self.nodes[node_id] = Node(node_id, features)\n",
    "\n",
    "    def add_edge(self, source_node, target_node):\n",
    "        self.nodes[source_node].adj_list.append(target_node)\n",
    "        self.nodes[target_node].adj_list.append(source_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "graph = Graph()\n",
    "\n",
    "# Add nodes with feature vectors\n",
    "node1_features = {'feature1': 10, 'feature2': 5}\n",
    "node2_features = {'feature1': 20, 'feature2': 8}\n",
    "node3_features = {'feature1': 15, 'feature2': 12}\n",
    "\n",
    "graph.add_node(1, node1_features)\n",
    "graph.add_node(2, node2_features)\n",
    "graph.add_node(3, node3_features)\n",
    "\n",
    "# Add edges between nodes\n",
    "graph.add_edge(1, 2)\n",
    "graph.add_edge(1, 3)\n",
    "graph.add_edge(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 1\n",
      "Adjacent Nodes: [2, 3]\n",
      "Feature Vector: {'feature1': 10, 'feature2': 5}\n",
      "\n",
      "Node ID: 2\n",
      "Adjacent Nodes: [1, 3]\n",
      "Feature Vector: {'feature1': 20, 'feature2': 8}\n",
      "\n",
      "Node ID: 3\n",
      "Adjacent Nodes: [1, 2]\n",
      "Feature Vector: {'feature1': 15, 'feature2': 12}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access node information and feature vectors\n",
    "for node_id, node in graph.nodes.items():\n",
    "    print(\"Node ID:\", node_id)\n",
    "    print(\"Adjacent Nodes:\", node.adj_list)\n",
    "    print(\"Feature Vector:\", node.features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125927\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def json_file_to_dict(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON data.\")\n",
    "        return {}\n",
    "\n",
    "# Replace 'path/to/your/file.json' with the actual path to your JSON file\n",
    "json_file_path = 'mem_event_data.json'\n",
    "resulting_dict = json_file_to_dict(json_file_path)\n",
    "\n",
    "print(len(resulting_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125927\n",
      "41680902 ['57156172', 'prznglytjbcc', '56638132', '83163042', '60596812', 'prznglythbwb', '83165162', '81571752', '83163902', '63797862', 'prznglytgbzb', '83164282', '83164872', '105368072', '83163152']\n",
      "39583752 ['230558829', '212224802', '196111562', '207525432', '221666147', '202963982', '230973467', '233942526', '207525342', '224662883', '206206232', '224934222', '224524678', 'zpvmrlyvfbsb', '202395152', '231167912', '209101112', '199390212', '231631390', '212538492', '230388729', 'fnvkslyvhbmb', '231144155', '230973632', '204717812', '224729929', '225613293', '230678172', 'qvvktlyvhbzb', '224448775', '230886204', '197775552']\n",
      "12320779 ['230288760', 'drmwwhysqbvb', 'pdbfdlyvjblc', '227888680', '234195139', '230275055', '230783910', '234531526']\n"
     ]
    }
   ],
   "source": [
    "ct=0\n",
    "print(len(resulting_dict))\n",
    "for i in resulting_dict:\n",
    "    print(i,resulting_dict[i])\n",
    "    ct+=1\n",
    "    if ct>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100741\n",
      "25186\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Example dictionary\n",
    "#data_dict = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5, \"f\": 6, \"g\": 7, \"h\": 8, \"i\": 9, \"j\": 10}\n",
    "\n",
    "# Convert the dictionary to a list of key-value pairs\n",
    "data_list = list(resulting_dict.items())\n",
    "\n",
    "# Shuffle the list in random order\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# Calculate the split point for 80% training data\n",
    "split_point = int(0.8 * len(data_list))\n",
    "\n",
    "# Create separate lists for training and testing data\n",
    "train_data_list = data_list[:split_point]\n",
    "test_data_list = data_list[split_point:]\n",
    "\n",
    "# Convert the lists back to dictionaries\n",
    "train_data_dict = dict(train_data_list)\n",
    "test_data_dict = dict(test_data_list)\n",
    "\n",
    "#print(\"Training data dictionary:\")\n",
    "print(len(train_data_dict))\n",
    "#print(\"\\nTesting data dictionary:\")\n",
    "print(len(test_data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now take the 50% for create the NN and 25% for +ve link and 25% for -ve link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50370\n",
      "50371\n"
     ]
    }
   ],
   "source": [
    "# Convert the dictionary to a list of key-value pairs\n",
    "data_list = list(train_data_dict.items())\n",
    "\n",
    "# Shuffle the list in random order\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# Calculate the split point for 50% training data\n",
    "split_point = int(0.5 * len(data_list))\n",
    "\n",
    "# Create separate lists for training and testing data\n",
    "train_NN = data_list[:split_point]\n",
    "train_remain = data_list[split_point:]\n",
    "\n",
    "# Convert the lists back to dictionaries\n",
    "train_NN = dict(train_NN)\n",
    "train_remain = dict(train_remain)\n",
    "\n",
    "#print(\"Training data dictionary:\")\n",
    "print(len(train_NN))\n",
    "#print(\"\\nTesting data dictionary:\")\n",
    "print(len(train_remain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50371\n",
      "107982252 ['qrdxscyrnbtb', '173536382', '166744122', '172216702']\n",
      "11432901 ['221866742', '221889701', '221719228', '222556887', '202952682', '221106389', '182085762', '222835336', '221975123', '226713208', '222470368', '220425290', '221014905', '222158836', '221131104', '221799468', '220701040', '231275611', '220882381', '222546998', '222325593', '221915853', '228757197', '222890407', '224563857', '230092113', '227417951']\n",
      "23681421 ['230898861', '222619660', '234178065']\n"
     ]
    }
   ],
   "source": [
    "ct=0\n",
    "print(len(train_remain))\n",
    "for i in train_remain:\n",
    "    print(i,train_remain[i])\n",
    "    ct+=1\n",
    "    if ct>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316972\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def create_adjacency_list(data):\n",
    "    adjacency_list = defaultdict(list)\n",
    "    for node, neighbors in data.items():\n",
    "        node = str(node)  # Convert node to string if it's not already\n",
    "        for neighbor in neighbors:\n",
    "            neighbor = str(neighbor)  # Convert neighbor to string if it's not already\n",
    "            adjacency_list[node].append(neighbor)\n",
    "            adjacency_list[neighbor].append(node)  # Add the reverse edge for bidirectional relationship\n",
    "    return adjacency_list\n",
    "\n",
    "adjacency_list = create_adjacency_list(train_NN)\n",
    "print(len(adjacency_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316972\n",
      "5650922 ['14359838', 'ncwmqyqgbcc', '228263078', '228120384', '233241604', '230009285', '228858987', '232558168', '230795582', '228357480']\n",
      "14359838 ['5650922', '7092500']\n",
      "ncwmqyqgbcc ['5650922', '7092500', '43872912']\n"
     ]
    }
   ],
   "source": [
    "ct=0\n",
    "print(len(adjacency_list))\n",
    "for i in adjacency_list:\n",
    "    print(i,adjacency_list[i])\n",
    "    ct+=1\n",
    "    if ct>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Replace 'data.json' with the actual path to your JSON file\n",
    "# with open('event_mem_adjacency_data.json') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# def create_adjacency_list(data):\n",
    "#     adjacency_list = defaultdict(list)\n",
    "#     for node, neighbors in data.items():\n",
    "#         node = str(node)  # Convert node to string if it's not already\n",
    "#         for neighbor in neighbors:\n",
    "#             neighbor = str(neighbor)  # Convert neighbor to string if it's not already\n",
    "#             adjacency_list[node].append(neighbor)\n",
    "#             adjacency_list[neighbor].append(node)  # Add the reverse edge for bidirectional relationship\n",
    "#     return adjacency_list\n",
    "\n",
    "# adjacency_list = create_adjacency_list(data)\n",
    "# print(len(adjacency_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def json_file_to_dict(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#             return data\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"Error: File not found.\")\n",
    "#         return {}\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(\"Error: Invalid JSON data.\")\n",
    "#         return {}\n",
    "\n",
    "# # Replace 'path/to/your/file.json' with the actual path to your JSON file\n",
    "# json_file_path = 'event_mem_adjacency_data.json'\n",
    "# resulting_dict = json_file_to_dict(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Read the JSON file with feature vectors\n",
    "# with open('mem_final_data.json', 'r') as f:\n",
    "#     feature_vectors_json = json.load(f)\n",
    "\n",
    "# # Create a dictionary to store the node features\n",
    "# node_features = {}\n",
    "\n",
    "# # Convert JSON data to a dictionary of node features\n",
    "# for node_id, feature_vector in feature_vectors_json.items():\n",
    "#     node_features[node_id] = feature_vector\n",
    "\n",
    "# Assuming you also have the adjacency list stored in a variable called adjacency_list\n",
    "\n",
    "# Create dictionaries to map both integer and string node IDs to unique integer indices\n",
    "id_mapping = {}\n",
    "reverse_id_mapping = {}\n",
    "index = 0\n",
    "for node_id in adjacency_list.keys():\n",
    "    if node_id not in id_mapping:\n",
    "        id_mapping[node_id] = index\n",
    "        reverse_id_mapping[index] = node_id\n",
    "        index += 1\n",
    "    for neighbor in adjacency_list[node_id]:\n",
    "        if neighbor not in id_mapping:\n",
    "            id_mapping[neighbor] = index\n",
    "            reverse_id_mapping[index] = neighbor\n",
    "            index += 1\n",
    "\n",
    "# Convert adjacency list to edge indices\n",
    "edges = []\n",
    "for src, neighbors in adjacency_list.items():\n",
    "    src_idx = id_mapping[src]\n",
    "    for tgt in neighbors:\n",
    "        tgt_idx = id_mapping[tgt]\n",
    "        edges.append([src_idx, tgt_idx])\n",
    "\n",
    "edges = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "# Create feature tensor\n",
    "# features_list = []\n",
    "# for node_id, feature_vector in node_features.items():\n",
    "#     features_list.append(feature_vector)\n",
    "\n",
    "# features_tensor = torch.tensor(features_list, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ...,   5613,  50623, 137270],\n",
       "        [     1,      2,      3,  ..., 183912,  50605, 137260]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(edges[0]))\n",
    "edges[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316972\n",
      "0 5650922\n",
      "1 14359838\n",
      "2 ncwmqyqgbcc\n"
     ]
    }
   ],
   "source": [
    "ct=0\n",
    "print(len(reverse_id_mapping))\n",
    "for i in reverse_id_mapping:\n",
    "    print(i,reverse_id_mapping[i])\n",
    "    ct+=1\n",
    "    if ct>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5038\n"
     ]
    }
   ],
   "source": [
    "# Convert the dictionary to a list of key-value pairs\n",
    "data_list = list(train_remain.items())\n",
    "\n",
    "# Shuffle the list in random order\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# Calculate the split point for 50% training data\n",
    "split_point = int(0.8 * len(data_list))\n",
    "\n",
    "# Create separate lists for training and testing data\n",
    "#train_NN = data_list[:split_point]\n",
    "train_remain = data_list[split_point:]\n",
    "\n",
    "# Convert the lists back to dictionaries\n",
    "#train_NN = dict(train_NN)\n",
    "train_remain = dict(train_remain)\n",
    "\n",
    "#print(\"Training data dictionary:\")\n",
    "#print(len(train_NN))\n",
    "#print(\"\\nTesting data dictionary:\")\n",
    "print(len(train_remain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59093\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def create_adjacency_list(data):\n",
    "    adjacency_train_remain = defaultdict(list)\n",
    "    for node, neighbors in data.items():\n",
    "        node = str(node)  # Convert node to string if it's not already\n",
    "        for neighbor in neighbors:\n",
    "            neighbor = str(neighbor)  # Convert neighbor to string if it's not already\n",
    "            adjacency_train_remain[node].append(neighbor)\n",
    "            adjacency_train_remain[neighbor].append(node)  # Add the reverse edge for bidirectional relationship\n",
    "    return adjacency_train_remain\n",
    "\n",
    "train_remain = create_adjacency_list(train_remain)\n",
    "print(len(train_remain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59093\n",
      "187484361 ['qcvqpkythblc']\n",
      "qcvqpkythblc ['187484361']\n",
      "185836289 ['220189134', 'qclkmlyvcbtb', '227646273']\n"
     ]
    }
   ],
   "source": [
    "ct=0\n",
    "print(len(train_remain))\n",
    "for i in train_remain:\n",
    "    print(i,train_remain[i])\n",
    "    ct+=1\n",
    "    if ct>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import torch\n",
    "\n",
    "# # Create positive examples from the existing connections in the adjacency list\n",
    "# positive_examples = []\n",
    "# for src, neighbors in train_remain.items():\n",
    "#     for tgt in neighbors:\n",
    "#         positive_examples.append((src, tgt))\n",
    "\n",
    "# # List all possible node pairs (members and events) for link prediction\n",
    "# all_nodes = list(train_remain.keys())\n",
    "# all_pairs = [(src, tgt) for src in all_nodes for tgt in all_nodes]\n",
    "\n",
    "# # Create negative examples by sampling random unconnected node pairs\n",
    "# negative_examples = []\n",
    "# for src, tgt in all_pairs:\n",
    "#     if (src, tgt) not in positive_examples:\n",
    "#         negative_examples.append((src, tgt))\n",
    "\n",
    "# # Create target labels for positive and negative examples\n",
    "# target_list = [1] * len(positive_examples) + [0] * len(negative_examples)\n",
    "\n",
    "# # Convert the target_list to a tensor\n",
    "# target_tensor = torch.tensor(target_list, dtype=torch.float)\n",
    "\n",
    "# print(\"Positive Examples:\", len(positive_examples))\n",
    "# print(\"Negative Examples:\",len(negative_examples))\n",
    "# print(\"Target Tensor:\", len(target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_16116\\3072851439.py:15: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  src, tgt = random.sample(train_remain.keys(), 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Examples: 141554\n",
      "Negative Examples: 141554\n",
      "Target Tensor: 283108\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Create positive examples from the existing connections in the adjacency list\n",
    "positive_examples = []\n",
    "for src, neighbors in train_remain.items():\n",
    "    for tgt in neighbors:\n",
    "        positive_examples.append((src, tgt))\n",
    "\n",
    "# Sample negative examples by randomly pairing unconnected nodes\n",
    "negative_examples = []\n",
    "while len(negative_examples) < len(positive_examples):\n",
    "    src, tgt = random.sample(train_remain.keys(), 2)\n",
    "    if (src, tgt) not in positive_examples:\n",
    "        negative_examples.append((src, tgt))\n",
    "\n",
    "# Create target labels for positive and negative examples\n",
    "target_list = [1] * len(positive_examples) + [0] * len(negative_examples)\n",
    "\n",
    "# Convert the target_list to a tensor\n",
    "target_tensor = torch.tensor(target_list, dtype=torch.float)\n",
    "\n",
    "print(\"Positive Examples:\", len(positive_examples))\n",
    "print(\"Negative Examples:\",len(negative_examples))\n",
    "print(\"Target Tensor:\", len(target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file with feature vectors\n",
    "with open('event_data.json', 'r') as f:\n",
    "    feature_vectors_event = json.load(f)\n",
    "    \n",
    "with open('mem_final_data.json', 'r') as f:\n",
    "    feature_vectors_mem = json.load(f)\n",
    "# Create a dictionary to store the node features\n",
    "node_features_event = {}\n",
    "node_features_mem = {}\n",
    "\n",
    "# Convert JSON data to a dictionary of node features\n",
    "for node_id, feature_vector in feature_vectors_event.items():\n",
    "    node_features_event[node_id] = feature_vector\n",
    "for node_id, feature_vector in feature_vectors_mem.items():\n",
    "    node_features_mem[node_id] = feature_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_member_feature = [0.0] * 500  # Replace num_features_member with the actual dimension of member feature vectors\n",
    "default_event_feature = [0.0] * 500  # Replace num_features_event with the actual dimension of event feature vectors\n",
    "# Create feature tensor\n",
    "features_list = []\n",
    "for node in adjacency_list:\n",
    "#    if node.startswith(\"member\"):\n",
    "    member_feature = node_features_mem.get(node, default_member_feature)\n",
    "    features_list.append(member_feature)\n",
    "#    elif node.startswith(\"event\"):\n",
    "    event_feature = node_features_event.get(node, default_event_feature)\n",
    "    features_list.append(event_feature)\n",
    "\n",
    "feature_tensor = torch.tensor(features_list, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data object\n",
    "data = Data(x=feature_tensor, edge_index=edges, y=target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[633944, 500], edge_index=[2, 1567144], y=[283108])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers):\n",
    "        super(SimpleGCN, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(GCNConv(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.conv_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.output_layer = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = F.relu(conv_layer(x, edge_index))\n",
    "        x = self.output_layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Assume you have defined the feature_tensor, edge_index, and other necessary inputs\n",
    "input_dim = feature_tensor.size(1)  # Dimension of the node feature vectors\n",
    "hidden_dim = 64\n",
    "num_classes = 2  # Adjust this based on your specific task\n",
    "num_layers = 5\n",
    "\n",
    "# Instantiate the GNN model\n",
    "model = SimpleGCN(input_dim, hidden_dim, num_classes, num_layers)\n",
    "\n",
    "# Pass the feature_tensor and edge_index through the GNN model\n",
    "output = model(feature_tensor, edges)\n",
    "# Now, you can use the output for further processing or make predictions for your specific task (e.g., link prediction, node classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633944"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ceating test data\n",
    "\n",
    "default_member_feature = [0.0] * 500  # Replace num_features_member with the actual dimension of member feature vectors\n",
    "default_event_feature = [0.0] * 500  # Replace num_features_event with the actual dimension of event feature vectors\n",
    "# Create feature tensor\n",
    "test_features_list = []\n",
    "for node in test_data_dict:\n",
    "#    if node.startswith(\"member\"):\n",
    "    member_feature = node_features_mem.get(node, default_member_feature)\n",
    "    test_features_list.append(member_feature)\n",
    "#    elif node.startswith(\"event\"):\n",
    "    event_feature = node_features_event.get(node, default_event_feature)\n",
    "    test_features_list.append(event_feature)\n",
    "\n",
    "#feature_tensor = torch.tensor(features_list, dtype=torch.float)\n",
    "\n",
    "# Create a new Data object for the test set with converted indices and features\n",
    "\n",
    "test_features_tensor = torch.tensor(test_features_list, dtype=torch.float)\n",
    "\n",
    "# Create the Data object for the test set\n",
    "test_data_object = Data(x=test_features_tensor, edge_index=edges, y=None)  # Assuming you don't have ground truth labels for the test set\n",
    "\n",
    "# Evaluate the GNN model on the test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_predictions = model(test_data_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[50372, 500], edge_index=[2, 1567144])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a trained GNN model and a test_data list containing event and member IDs\n",
    "\n",
    "# # Convert event and member IDs to indices\n",
    "# test_indices = [nodes.index(node_id) for node_id in test_data]\n",
    "\n",
    "# # Create a new Data object for the test set with converted indices and features\n",
    "# test_features_list = [features_list[idx] for idx in test_indices]\n",
    "# test_features_tensor = torch.tensor(test_features_list, dtype=torch.float)\n",
    "\n",
    "# # Assuming you have the ground truth labels for the test set as test_target_list\n",
    "# test_target_tensor = torch.tensor(test_target_list, dtype=torch.float)\n",
    "\n",
    "# # Create the Data object for the test set\n",
    "# test_data_object = Data(x=test_features_tensor, edge_index=edges, y=test_target_tensor)\n",
    "\n",
    "# Evaluate the GNN model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(test_data_object)\n",
    "\n",
    "# If you have ground truth labels, calculate evaluation metrics\n",
    "# For example, if the target tensor is binary (link exists or not), you can use binary classification metrics.\n",
    "# You can use libraries like scikit-learn to compute these metrics.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "test_predictions_numpy = test_predictions.numpy()\n",
    "test_predictions_binary = (test_predictions_numpy > 0.5).astype(int)\n",
    "test_target_numpy = test_target_tensor.numpy()\n",
    "\n",
    "accuracy = accuracy_score(test_target_numpy, test_predictions_binary)\n",
    "precision = precision_score(test_target_numpy, test_predictions_binary)\n",
    "recall = recall_score(test_target_numpy, test_predictions_binary)\n",
    "f1 = f1_score(test_target_numpy, test_predictions_binary)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([283108])) that is different to the input size (torch.Size([633944, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Assuming you have labels for positive/negative examples\u001b[39;00m\n\u001b[0;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3074\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3072\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3076\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3077\u001b[0m     )\n\u001b[0;32m   3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3080\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([283108])) that is different to the input size (torch.Size([633944, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Assuming you have prepared your dataset with existing nodes, features, and edges\n",
    "\n",
    "# Create the GNN model\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Prepare the data for the GNN model\n",
    "data = Data(x=feature_tensor, edge_index=edges,y=target_tensor)\n",
    "\n",
    "# Create the GNN model and optimizer\n",
    "model = GNNModel(in_channels=500, hidden_channels=64, out_channels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out, data.y) # Assuming you have labels for positive/negative examples\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    predictions = out[test_data_object] # Get predictions for test set\n",
    "\n",
    "# Perform link prediction for a new node\n",
    "# new_node_features = torch.tensor([[...]])  # Replace with the feature vector of the new node\n",
    "# new_node_edge_index = torch.tensor([[existing_node_id, new_node_id]])  # Add edge between the new node and an existing node\n",
    "# new_node_predictions = model(new_node_features, new_node_edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, SortPooling, SumPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, SortPooling, SumPooling\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    GCN Model\n",
    "\n",
    "    Attributes:\n",
    "        num_layers(int): num of gcn layers\n",
    "        hidden_units(int): num of hidden units\n",
    "        gcn_type(str): type of gcn layer, 'gcn' for GraphConv and 'sage' for SAGEConv\n",
    "        pooling_type(str): type of graph pooling to get subgraph representation\n",
    "                           'sum' for sum pooling and 'center' for center pooling.\n",
    "        node_attributes(Tensor, optional): node attribute\n",
    "        edge_weights(Tensor, optional): edge weight\n",
    "        node_embedding(Tensor, optional): pre-trained node embedding\n",
    "        use_embedding(bool, optional): whether to use node embedding. Note that if 'use_embedding' is set True\n",
    "                             and 'node_embedding' is None, will automatically randomly initialize node embedding.\n",
    "        num_nodes(int, optional): num of nodes\n",
    "        dropout(float, optional): dropout rate\n",
    "        max_z(int, optional): default max vocab size of node labeling, default 1000.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        hidden_units,\n",
    "        gcn_type=\"gcn\",\n",
    "        pooling_type=\"sum\",\n",
    "        node_attributes=None,\n",
    "        edge_weights=None,\n",
    "        node_embedding=None,\n",
    "        use_embedding=False,\n",
    "        num_nodes=None,\n",
    "        dropout=0.5,\n",
    "        max_z=1000,\n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.pooling_type = pooling_type\n",
    "        self.use_attribute = False if node_attributes is None else True\n",
    "        self.use_embedding = use_embedding\n",
    "        self.use_edge_weight = False if edge_weights is None else True\n",
    "\n",
    "        self.z_embedding = nn.Embedding(max_z, hidden_units)\n",
    "        if node_attributes is not None:\n",
    "            self.node_attributes_lookup = nn.Embedding.from_pretrained(\n",
    "                node_attributes\n",
    "            )\n",
    "            self.node_attributes_lookup.weight.requires_grad = False\n",
    "        if edge_weights is not None:\n",
    "            self.edge_weights_lookup = nn.Embedding.from_pretrained(\n",
    "                edge_weights\n",
    "            )\n",
    "            self.edge_weights_lookup.weight.requires_grad = False\n",
    "        if node_embedding is not None:\n",
    "            self.node_embedding = nn.Embedding.from_pretrained(node_embedding)\n",
    "            self.node_embedding.weight.requires_grad = False\n",
    "        elif use_embedding:\n",
    "            self.node_embedding = nn.Embedding(num_nodes, hidden_units)\n",
    "\n",
    "        initial_dim = hidden_units\n",
    "        if self.use_attribute:\n",
    "            initial_dim += self.node_attributes_lookup.embedding_dim\n",
    "        if self.use_embedding:\n",
    "            initial_dim += self.node_embedding.embedding_dim\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        if gcn_type == \"gcn\":\n",
    "            self.layers.append(\n",
    "                GraphConv(initial_dim, hidden_units, allow_zero_in_degree=True)\n",
    "            )\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.layers.append(\n",
    "                    GraphConv(\n",
    "                        hidden_units, hidden_units, allow_zero_in_degree=True\n",
    "                    )\n",
    "                )\n",
    "        elif gcn_type == \"sage\":\n",
    "            self.layers.append(\n",
    "                SAGEConv(initial_dim, hidden_units, aggregator_type=\"gcn\")\n",
    "            )\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.layers.append(\n",
    "                    SAGEConv(hidden_units, hidden_units, aggregator_type=\"gcn\")\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Gcn type error.\")\n",
    "\n",
    "        self.linear_1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.linear_2 = nn.Linear(hidden_units, 1)\n",
    "        if pooling_type != \"sum\":\n",
    "            raise ValueError(\"Pooling type error.\")\n",
    "        self.pooling = SumPooling()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, g, z, node_id=None, edge_id=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            g(DGLGraph): the graph\n",
    "            z(Tensor): node labeling tensor, shape [N, 1]\n",
    "            node_id(Tensor, optional): node id tensor, shape [N, 1]\n",
    "            edge_id(Tensor, optional): edge id tensor, shape [E, 1]\n",
    "        Returns:\n",
    "            x(Tensor): output tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        z_emb = self.z_embedding(z)\n",
    "\n",
    "        if self.use_attribute:\n",
    "            x = self.node_attributes_lookup(node_id)\n",
    "            x = torch.cat([z_emb, x], 1)\n",
    "        else:\n",
    "            x = z_emb\n",
    "\n",
    "        if self.use_edge_weight:\n",
    "            edge_weight = self.edge_weights_lookup(edge_id)\n",
    "        else:\n",
    "            edge_weight = None\n",
    "\n",
    "        if self.use_embedding:\n",
    "            n_emb = self.node_embedding(node_id)\n",
    "            x = torch.cat([x, n_emb], 1)\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(g, x, edge_weight=edge_weight)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.layers[-1](g, x, edge_weight=edge_weight)\n",
    "\n",
    "        x = self.pooling(g, x)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DGCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    An end-to-end deep learning architecture for graph classification.\n",
    "    paper link: https://muhanzhang.github.io/papers/AAAI_2018_DGCNN.pdf\n",
    "\n",
    "    Attributes:\n",
    "        num_layers(int): num of gcn layers\n",
    "        hidden_units(int): num of hidden units\n",
    "        k(int, optional): The number of nodes to hold for each graph in SortPooling.\n",
    "        gcn_type(str): type of gcn layer, 'gcn' for GraphConv and 'sage' for SAGEConv\n",
    "        node_attributes(Tensor, optional): node attribute\n",
    "        edge_weights(Tensor, optional): edge weight\n",
    "        node_embedding(Tensor, optional): pre-trained node embedding\n",
    "        use_embedding(bool, optional): whether to use node embedding. Note that if 'use_embedding' is set True\n",
    "                             and 'node_embedding' is None, will automatically randomly initialize node embedding.\n",
    "        num_nodes(int, optional): num of nodes\n",
    "        dropout(float, optional): dropout rate\n",
    "        max_z(int, optional): default max vocab size of node labeling, default 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        hidden_units,\n",
    "        k=10,\n",
    "        gcn_type=\"gcn\",\n",
    "        node_attributes=None,\n",
    "        edge_weights=None,\n",
    "        node_embedding=None,\n",
    "        use_embedding=False,\n",
    "        num_nodes=None,\n",
    "        dropout=0.5,\n",
    "        max_z=1000,\n",
    "    ):\n",
    "        super(DGCNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_attribute = False if node_attributes is None else True\n",
    "        self.use_embedding = use_embedding\n",
    "        self.use_edge_weight = False if edge_weights is None else True\n",
    "\n",
    "        self.z_embedding = nn.Embedding(max_z, hidden_units)\n",
    "\n",
    "        if node_attributes is not None:\n",
    "            self.node_attributes_lookup = nn.Embedding.from_pretrained(\n",
    "                node_attributes\n",
    "            )\n",
    "            self.node_attributes_lookup.weight.requires_grad = False\n",
    "        if edge_weights is not None:\n",
    "            self.edge_weights_lookup = nn.Embedding.from_pretrained(\n",
    "                edge_weights\n",
    "            )\n",
    "            self.edge_weights_lookup.weight.requires_grad = False\n",
    "        if node_embedding is not None:\n",
    "            self.node_embedding = nn.Embedding.from_pretrained(node_embedding)\n",
    "            self.node_embedding.weight.requires_grad = False\n",
    "        elif use_embedding:\n",
    "            self.node_embedding = nn.Embedding(num_nodes, hidden_units)\n",
    "\n",
    "        initial_dim = hidden_units\n",
    "        if self.use_attribute:\n",
    "            initial_dim += self.node_attributes_lookup.embedding_dim\n",
    "        if self.use_embedding:\n",
    "            initial_dim += self.node_embedding.embedding_dim\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        if gcn_type == \"gcn\":\n",
    "            self.layers.append(\n",
    "                GraphConv(initial_dim, hidden_units, allow_zero_in_degree=True)\n",
    "            )\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.layers.append(\n",
    "                    GraphConv(\n",
    "                        hidden_units, hidden_units, allow_zero_in_degree=True\n",
    "                    )\n",
    "                )\n",
    "            self.layers.append(\n",
    "                GraphConv(hidden_units, 1, allow_zero_in_degree=True)\n",
    "            )\n",
    "        elif gcn_type == \"sage\":\n",
    "            self.layers.append(\n",
    "                SAGEConv(initial_dim, hidden_units, aggregator_type=\"gcn\")\n",
    "            )\n",
    "            for _ in range(num_layers - 1):\n",
    "                self.layers.append(\n",
    "                    SAGEConv(hidden_units, hidden_units, aggregator_type=\"gcn\")\n",
    "                )\n",
    "            self.layers.append(SAGEConv(hidden_units, 1, aggregator_type=\"gcn\"))\n",
    "        else:\n",
    "            raise ValueError(\"Gcn type error.\")\n",
    "\n",
    "        self.pooling = SortPooling(k=k)\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_units * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            1, conv1d_channels[0], conv1d_kws[0], conv1d_kws[0]\n",
    "        )\n",
    "        self.maxpool1d = nn.MaxPool1d(2, 2)\n",
    "        self.conv_2 = nn.Conv1d(\n",
    "            conv1d_channels[0], conv1d_channels[1], conv1d_kws[1], 1\n",
    "        )\n",
    "        dense_dim = int((k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.linear_1 = nn.Linear(dense_dim, 128)\n",
    "        self.linear_2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, g, z, node_id=None, edge_id=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            g(DGLGraph): the graph\n",
    "            z(Tensor): node labeling tensor, shape [N, 1]\n",
    "            node_id(Tensor, optional): node id tensor, shape [N, 1]\n",
    "            edge_id(Tensor, optional): edge id tensor, shape [E, 1]\n",
    "        Returns:\n",
    "            x(Tensor): output tensor\n",
    "        \"\"\"\n",
    "        z_emb = self.z_embedding(z)\n",
    "        if self.use_attribute:\n",
    "            x = self.node_attributes_lookup(node_id)\n",
    "            x = torch.cat([z_emb, x], 1)\n",
    "        else:\n",
    "            x = z_emb\n",
    "        if self.use_edge_weight:\n",
    "            edge_weight = self.edge_weights_lookup(edge_id)\n",
    "        else:\n",
    "            edge_weight = None\n",
    "\n",
    "        if self.use_embedding:\n",
    "            n_emb = self.node_embedding(node_id)\n",
    "            x = torch.cat([x, n_emb], 1)\n",
    "\n",
    "        xs = [x]\n",
    "        for layer in self.layers:\n",
    "            out = torch.tanh(layer(g, xs[-1], edge_weight=edge_weight))\n",
    "            xs += [out]\n",
    "\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # SortPooling\n",
    "        x = self.pooling(g, x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.maxpool1d(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
